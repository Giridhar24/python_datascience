# Concept of the Day: Overfitting vs. Underfitting
# Explanation: When training a Machine Learning model, you want it to learn the pattern, not the noise.
# Underfitting (Too Simple): The model is too lazy. It draws a straight line through curved data. It has high "Bias."
# Overfitting (Too Complex): The model tries too hard. It memorizes every single dot, including random errors. It works perfectly on training data but fails on new data. It has high "Variance."
# Good Fit (Goldilocks): Captures the general trend without memorizing noise.